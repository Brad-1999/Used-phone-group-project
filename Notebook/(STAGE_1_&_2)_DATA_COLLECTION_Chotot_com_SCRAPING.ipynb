{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Subject: Data Scraping Code Submission."
      ],
      "metadata": {
        "id": "7ywf9i65rg_E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Date: October 15, 2024."
      ],
      "metadata": {
        "id": "GyUNtBlMFL5p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dear Professor Ilia Tetin,\n",
        "\n",
        "I am writing on behalf of our presentation team, which consists of two members:\n",
        "\n",
        "\n",
        "\n",
        "*   LE TRAN NHA TRAN - JASMINE (Student ID: 11285100M);\n",
        "\n",
        "*   DINH VAN LONG - BRAD (Student ID: 11285109M).\n"
      ],
      "metadata": {
        "id": "QvLBPB19eMgy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have decided to focus on the topic: \"Consumer Trends in the E-Commerce Platform (Chotot.com) for Used Cell Phones: Insights and Predictions\". This study will analyze consumer behavior, market dynamics, and emerging trends in the used smartphone sector, using a dataset of over 19,000 observations collected from Chotot.com. The data represents 63 provinces and cities across Vietnam, encompassing both urban and rural areas, ensuring a comprehensive understanding of the market.\n",
        "\n",
        "Chotot.com operates under the motto \"A Way to Your Wants\" (LinkedIn: https://www.linkedin.com/company/cho-tot/) and functions as a marketplace offering a wide variety of physical goods to Vietnamese consumers. For our research, we have specifically focused on the used smartphone category.\n",
        "\n",
        "\n",
        "Enclosed below is the data scraping code we developed, which extracts data from Chotot.com. This dataset serves as the foundation for our topic, which investigates pricing strategy trends within this second-hand marketplace."
      ],
      "metadata": {
        "id": "ietBAI5rH82N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "OtK3RfWoqCJV",
        "outputId": "20a7b976-8df5-4150-ab91-d011449740a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scrapy\n",
            "  Downloading Scrapy-2.12.0-py2.py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting scrapy-user-agents\n",
            "  Downloading scrapy_user_agents-0.1.1-py2.py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting Twisted>=21.7.0 (from scrapy)\n",
            "  Downloading twisted-24.10.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: cryptography>=37.0.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (43.0.3)\n",
            "Collecting cssselect>=0.9.1 (from scrapy)\n",
            "  Downloading cssselect-1.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting itemloaders>=1.0.1 (from scrapy)\n",
            "  Downloading itemloaders-1.3.2-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting parsel>=1.5.0 (from scrapy)\n",
            "  Downloading parsel-1.9.1-py2.py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: pyOpenSSL>=22.0.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (24.2.1)\n",
            "Collecting queuelib>=1.4.2 (from scrapy)\n",
            "  Downloading queuelib-1.7.0-py2.py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting service-identity>=18.1.0 (from scrapy)\n",
            "  Downloading service_identity-24.2.0-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting w3lib>=1.17.0 (from scrapy)\n",
            "  Downloading w3lib-2.2.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting zope.interface>=5.1.0 (from scrapy)\n",
            "  Downloading zope.interface-7.1.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting protego>=0.1.15 (from scrapy)\n",
            "  Downloading Protego-0.3.1-py2.py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting itemadapter>=0.1.0 (from scrapy)\n",
            "  Downloading itemadapter-0.9.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from scrapy) (24.2)\n",
            "Collecting tldextract (from scrapy)\n",
            "  Downloading tldextract-5.1.3-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: lxml>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (5.3.0)\n",
            "Requirement already satisfied: defusedxml>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from scrapy) (0.7.1)\n",
            "Collecting PyDispatcher>=2.0.5 (from scrapy)\n",
            "  Downloading PyDispatcher-2.0.7-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting user-agents (from scrapy-user-agents)\n",
            "  Downloading user_agents-2.2.0-py3-none-any.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=37.0.0->scrapy) (1.17.1)\n",
            "Collecting jmespath>=0.9.5 (from itemloaders>=1.0.1->scrapy)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from service-identity>=18.1.0->scrapy) (24.2.0)\n",
            "Requirement already satisfied: pyasn1 in /usr/local/lib/python3.10/dist-packages (from service-identity>=18.1.0->scrapy) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.10/dist-packages (from service-identity>=18.1.0->scrapy) (0.4.1)\n",
            "Collecting automat>=24.8.0 (from Twisted>=21.7.0->scrapy)\n",
            "  Downloading Automat-24.8.1-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting constantly>=15.1 (from Twisted>=21.7.0->scrapy)\n",
            "  Downloading constantly-23.10.4-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting hyperlink>=17.1.1 (from Twisted>=21.7.0->scrapy)\n",
            "  Downloading hyperlink-21.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting incremental>=24.7.0 (from Twisted>=21.7.0->scrapy)\n",
            "  Downloading incremental-24.7.2-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from Twisted>=21.7.0->scrapy) (4.12.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from zope.interface>=5.1.0->scrapy) (75.1.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from tldextract->scrapy) (3.10)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from tldextract->scrapy) (2.32.3)\n",
            "Collecting requests-file>=1.4 (from tldextract->scrapy)\n",
            "  Downloading requests_file-2.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from tldextract->scrapy) (3.16.1)\n",
            "Collecting ua-parser>=0.10.0 (from user-agents->scrapy-user-agents)\n",
            "  Downloading ua_parser-0.18.0-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=37.0.0->scrapy) (2.22)\n",
            "Requirement already satisfied: tomli in /usr/local/lib/python3.10/dist-packages (from incremental>=24.7.0->Twisted>=21.7.0->scrapy) (2.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract->scrapy) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract->scrapy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract->scrapy) (2024.8.30)\n",
            "Downloading Scrapy-2.12.0-py2.py3-none-any.whl (311 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scrapy_user_agents-0.1.1-py2.py3-none-any.whl (27 kB)\n",
            "Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
            "Downloading itemadapter-0.9.0-py3-none-any.whl (11 kB)\n",
            "Downloading itemloaders-1.3.2-py3-none-any.whl (12 kB)\n",
            "Downloading parsel-1.9.1-py2.py3-none-any.whl (17 kB)\n",
            "Downloading Protego-0.3.1-py2.py3-none-any.whl (8.5 kB)\n",
            "Downloading PyDispatcher-2.0.7-py3-none-any.whl (12 kB)\n",
            "Downloading queuelib-1.7.0-py2.py3-none-any.whl (13 kB)\n",
            "Downloading service_identity-24.2.0-py3-none-any.whl (11 kB)\n",
            "Downloading twisted-24.10.0-py3-none-any.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading w3lib-2.2.1-py3-none-any.whl (21 kB)\n",
            "Downloading zope.interface-7.1.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (254 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.2/254.2 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tldextract-5.1.3-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.9/104.9 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading user_agents-2.2.0-py3-none-any.whl (9.6 kB)\n",
            "Downloading Automat-24.8.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading constantly-23.10.4-py3-none-any.whl (13 kB)\n",
            "Downloading hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.6/74.6 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading incremental-24.7.2-py3-none-any.whl (20 kB)\n",
            "Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\n",
            "Downloading ua_parser-0.18.0-py2.py3-none-any.whl (38 kB)\n",
            "Installing collected packages: ua-parser, PyDispatcher, zope.interface, w3lib, user-agents, queuelib, protego, jmespath, itemadapter, incremental, hyperlink, cssselect, constantly, automat, Twisted, scrapy-user-agents, requests-file, parsel, tldextract, service-identity, itemloaders, scrapy\n",
            "Successfully installed PyDispatcher-2.0.7 Twisted-24.10.0 automat-24.8.1 constantly-23.10.4 cssselect-1.2.0 hyperlink-21.0.0 incremental-24.7.2 itemadapter-0.9.0 itemloaders-1.3.2 jmespath-1.0.1 parsel-1.9.1 protego-0.3.1 queuelib-1.7.0 requests-file-2.1.0 scrapy-2.12.0 scrapy-user-agents-0.1.1 service-identity-24.2.0 tldextract-5.1.3 ua-parser-0.18.0 user-agents-2.2.0 w3lib-2.2.1 zope.interface-7.1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install -U scrapy scrapy-user-agents"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import scrapy\n",
        "import json\n",
        "from datetime import datetime\n",
        "from dataclasses import dataclass, asdict\n",
        "from scrapy.crawler import CrawlerProcess"
      ],
      "metadata": {
        "id": "wSFqrPV_qPvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of pages the scraper will attempt to crawl\n",
        "NUM_PAGES = 10000\n",
        "\n",
        "# Scrapy settings dictionary\n",
        "settings = {\n",
        "    # Define output format and file for the scraped data\n",
        "    \"FEEDS\": {\"posts.jsonl\": {\"format\": \"jsonlines\"}},  # Save data to `posts.jsonl` in JSON Lines format\n",
        "    \"FEED_EXPORT_ENCODING\": \"utf-8\",  # Use UTF-8 encoding for the output file\n",
        "\n",
        "    # Middleware settings for handling user agents\n",
        "    \"DOWNLOADER_MIDDLEWARES\": {\n",
        "        # Disable the default UserAgentMiddleware\n",
        "        \"scrapy.downloadermiddlewares.useragent.UserAgentMiddleware\": None,\n",
        "        # Enable a custom middleware for rotating random user agents\n",
        "        \"scrapy_user_agents.middlewares.RandomUserAgentMiddleware\": 400,\n",
        "    },\n",
        "\n",
        "    # Default headers sent with each request\n",
        "    \"DEFAULT_REQUEST_HEADERS\": {\n",
        "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",  # Accept a variety of content types\n",
        "        \"Accept-Language\": \"en\",  # Set preferred language to English\n",
        "    },\n",
        "\n",
        "    # Performance tuning\n",
        "    \"CONCURRENT_REQUESTS\": 128,  # Number of concurrent requests allowed\n",
        "    \"COOKIES_ENABLED\": False,  # Disable cookies for better performance and fewer restrictions\n",
        "    \"TELNETCONSOLE_ENABLED\": False,  # Disable Telnet console for security and simplicity\n",
        "    \"DOWNLOAD_DELAY\": 0.1,  # Delay between consecutive requests (100ms)\n",
        "\n",
        "    # Crawler behavior\n",
        "    \"ROBOTSTXT_OBEY\": True,  # Respect `robots.txt` rules to avoid violating site policies\n",
        "\n",
        "    # Logging settings\n",
        "    \"LOG_LEVEL\": \"INFO\",  # Set log verbosity to show only informational messages and above\n",
        "}\n",
        "\n",
        "# Disable all logging to suppress unnecessary output\n",
        "logging.disable(logging.CRITICAL)"
      ],
      "metadata": {
        "id": "5l9LIsNMqyhS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code defines a dataclass called PhoneCrawlerItem, structuring the data collected during web crawling."
      ],
      "metadata": {
        "id": "iDqrhaFuG7yN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10,000 is the maximum number of pages the scraper will attempt to crawl."
      ],
      "metadata": {
        "id": "YbA9gNZbYYK-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the dataclass decorator\n",
        "from dataclasses import dataclass\n",
        "\n",
        "# Define a PhoneCrawlerItem class to represent a scraped item\n",
        "@dataclass\n",
        "class PhoneCrawlerItem:\n",
        "    # Unique identifier for the listing\n",
        "    listing_id: str  # Type: String\n",
        "\n",
        "    # URL of the webpage where the listing was scraped\n",
        "    url: str  # Type: String\n",
        "\n",
        "    # Raw or processed content of the listing\n",
        "    content: dict  # Type: Dictionary (key-value pairs)\n",
        "\n",
        "    # Date when the data was crawled\n",
        "    crawl_date: str  # Type: String\n",
        "\n",
        "    # Source or website from which the data was scraped\n",
        "    source: str  # Type: String"
      ],
      "metadata": {
        "id": "vQA7v_DWHD49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This Scrapy spider, UsedSmartphoneChototSpier, is designed to scrape used smartphone listings from the Chotot website."
      ],
      "metadata": {
        "id": "Nb6XyTgnHRgd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UsedSmartphoneChototSpier(scrapy.Spider):\n",
        "    name = \"chotot\"  # Name of the spider\n",
        "    allowed_domains = [\"chotot.com\"]  # Domains the spider is allowed to crawl\n",
        "    start_urls = [\n",
        "        f\"https://www.chotot.com/mua-ban-dien-thoai?page={i}\" for i in range(NUM_PAGES)\n",
        "    ]  # List of URLs to start scraping from\n",
        "    crawl_date = datetime.now().strftime(r\"%Y-%m-%d\")  # Date of the crawl\n",
        "\n",
        "    def parse(self, response: scrapy.http.Response):\n",
        "    listings = json.loads(\n",
        "        response.xpath('//script[@id=\"__NEXT_DATA__\"]/text()').get()\n",
        "    )  # Extract JSON data from a specific script tag\n",
        "    urls = [\n",
        "        f\"{listing['list_id']}.htm\"\n",
        "        for listing in listings[\"props\"][\"pageProps\"][\"initialState\"][\"adlisting\"][\n",
        "            \"data\"\n",
        "        ][\"ads\"]\n",
        "    ]  # Extract listing IDs and create URLs\n",
        "    print(f\"Found {len(urls)} listings @ {response.url}\")\n",
        "    yield from response.follow_all(urls, self.parse_post)  # Follow each URL to parse post details\n",
        "\n",
        "    def parse_post(self, response):\n",
        "        json_content = response.xpath('//script[@id=\"__NEXT_DATA__\"]/text()').get()\n",
        "        json_content = json.loads(json_content)\n",
        "        results = {}\n",
        "        results[\"listing_id\"] = json_content[\"query\"][\"listId\"]\n",
        "        results[\"content\"] = {}\n",
        "        results[\"url\"] = json_content[\"props\"][\"canonicalUrl\"]\n",
        "        results[\"content\"][\"props\"] = {}\n",
        "        results[\"content\"][\"props\"][\"pageProps\"] = {}\n",
        "        results[\"content\"][\"props\"][\"pageProps\"][\"initialState\"] = {}\n",
        "        results[\"content\"][\"props\"][\"pageProps\"][\"initialState\"][\"adView\"] = {}\n",
        "        results[\"content\"][\"props\"][\"pageProps\"][\"initialState\"][\"adView\"][\n",
        "            \"adInfo\"\n",
        "        ] = {}\n",
        "        results[\"content\"][\"props\"][\"pageProps\"][\"initialState\"][\"adView\"][\"adInfo\"][\n",
        "            \"ad\"\n",
        "        ] = json_content[\"props\"][\"pageProps\"][\"initialState\"][\"adView\"][\"adInfo\"][\"ad\"]\n",
        "        results[\"content\"][\"props\"][\"pageProps\"][\"initialState\"][\"adView\"][\"adInfo\"][\n",
        "            \"ad_params\"\n",
        "        ] = json_content[\"props\"][\"pageProps\"][\"initialState\"][\"adView\"][\"adInfo\"][\n",
        "            \"ad_params\"\n",
        "        ]\n",
        "        results[\"content\"][\"props\"][\"pageProps\"][\"initialState\"][\"nav\"] = {}\n",
        "        results[\"content\"][\"props\"][\"pageProps\"][\"initialState\"][\"nav\"][\"navObj\"] = (\n",
        "            json_content[\"props\"][\"initialState\"][\"nav\"][\"navObj\"]\n",
        "        )\n",
        "        yield PhoneCrawlerItem(\n",
        "            listing_id=results[\"listing_id\"],\n",
        "            url=results[\"url\"],\n",
        "            content=results[\"content\"],\n",
        "            crawl_date=self.crawl_date,\n",
        "            source=\"chotot\",\n",
        "        )"
      ],
      "metadata": {
        "id": "Bu531ziCqmbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a CrawlerProcess instance with the defined settings\n",
        "process = CrawlerProcess(settings=settings)\n",
        "# Add the UsedSmartphoneChototSpier spider to the process\n",
        "process.crawl(UsedSmartphoneChototSpier)\n",
        "# Start the crawling process\n",
        "process.start()"
      ],
      "metadata": {
        "id": "RVMPAsLhqwBn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "662d9229-82ca-4891-f880-f509a1a7b307"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20 listings @ https://www.chotot.com/mua-ban-dien-thoai?page=2\n",
            "Found 20 listings @ https://www.chotot.com/mua-ban-dien-thoai?page=0\n",
            "Found 20 listings @ https://www.chotot.com/mua-ban-dien-thoai?page=5\n",
            "Found 20 listings @ https://www.chotot.com/mua-ban-dien-thoai?page=1\n",
            "Found 20 listings @ https://www.chotot.com/mua-ban-dien-thoai?page=4\n",
            "Found 20 listings @ https://www.chotot.com/mua-ban-dien-thoai?page=3\n",
            "Found 20 listings @ https://www.chotot.com/mua-ban-dien-thoai?page=6\n",
            "Found 20 listings @ https://www.chotot.com/mua-ban-dien-thoai?page=7\n",
            "Found 20 listings @ https://www.chotot.com/mua-ban-dien-thoai?page=8\n",
            "Found 20 listings @ https://www.chotot.com/mua-ban-dien-thoai?page=9\n",
            "Found 20 listings @ https://www.chotot.com/mua-ban-dien-thoai?page=15\n",
            "Found 20 listings @ https://www.chotot.com/mua-ban-dien-thoai?page=11\n",
            "Found 20 listings @ https://www.chotot.com/mua-ban-dien-thoai?page=12\n",
            "Found 20 listings @ https://www.chotot.com/mua-ban-dien-thoai?page=10\n",
            "Found 20 listings @ https://www.chotot.com/mua-ban-dien-thoai?page=13\n",
            "Found 20 listings @ https://www.chotot.com/mua-ban-dien-thoai?page=14\n",
            "Found 20 listings @ https://www.chotot.com/mua-ban-dien-thoai?page=19\n",
            "Found 20 listings @ https://www.chotot.com/mua-ban-dien-thoai?page=22\n",
            "Found 20 listings @ https://www.chotot.com/mua-ban-dien-thoai?page=20\n",
            "Found 20 listings @ https://www.chotot.com/mua-ban-dien-thoai?page=21\n",
            "Found 20 listings @ https://www.chotot.com/mua-ban-dien-thoai?page=18\n",
            "Found 20 listings @ https://www.chotot.com/mua-ban-dien-thoai?page=16\n",
            "Found 20 listings @ https://www.chotot.com/mua-ban-dien-thoai?page=23\n",
            "Found 20 listings @ https://www.chotot.com/mua-ban-dien-thoai?page=25\n",
            "Found 20 listings @ https://www.chotot.com/mua-ban-dien-thoai?page=17\n",
            "Found 20 listings @ https://www.chotot.com/mua-ban-dien-thoai?page=27\n",
            "Found 20 listings @ https://www.chotot.com/mua-ban-dien-thoai?page=24\n",
            "Found 20 listings @ https://www.chotot.com/mua-ban-dien-thoai?page=28\n",
            "Found 20 listings @ https://www.chotot.com/mua-ban-dien-thoai?page=29\n",
            "Found 20 listings @ https://www.chotot.com/mua-ban-dien-thoai?page=31\n",
            "Found 20 listings @ https://www.chotot.com/mua-ban-dien-thoai?page=30\n",
            "Found 20 listings @ https://www.chotot.com/mua-ban-dien-thoai?page=26\n",
            "Found 20 listings @ https://www.chotot.com/mua-ban-dien-thoai?page=34\n",
            "Found 20 listings @ https://www.chotot.com/mua-ban-dien-thoai?page=32\n",
            "Found 20 listings @ https://www.chotot.com/mua-ban-dien-thoai?page=33\n",
            "Found 20 listings @ https://www.chotot.com/mua-ban-dien-thoai?page=36\n",
            "Found 20 listings @ https://www.chotot.com/mua-ban-dien-thoai?page=35\n",
            "Found 20 listings @ https://www.chotot.com/mua-ban-dien-thoai?page=37\n",
            "Found 20 listings @ https://www.chotot.com/mua-ban-dien-thoai?page=39\n",
            "Found 20 listings @ https://www.chotot.com/mua-ban-dien-thoai?page=38\n",
            "Found 20 listings @ https://www.chotot.com/mua-ban-dien-thoai?page=40\n",
            "Found 20 listings @ https://www.chotot.com/mua-ban-dien-thoai?page=46\n",
            "Found 20 listings @ https://www.chotot.com/mua-ban-dien-thoai?page=43\n",
            "Found 20 listings @ https://www.chotot.com/mua-ban-dien-thoai?page=44\n",
            "Found 20 listings @ https://www.chotot.com/mua-ban-dien-thoai?page=45\n",
            "Found 20 listings @ https://www.chotot.com/mua-ban-dien-thoai?page=41\n",
            "Found 20 listings @ https://www.chotot.com/mua-ban-dien-thoai?page=42\n",
            "Found 20 listings @ https://www.chotot.com/mua-ban-dien-thoai?page=48\n",
            "Found 20 listings @ https://www.chotot.com/mua-ban-dien-thoai?page=49\n",
            "Found 20 listings @ https://www.chotot.com/mua-ban-dien-thoai?page=47\n",
            "Found 20 listings @ https://www.chotot.com/mua-ban-dien-thoai?page=50\n",
            "Found 20 listings @ https://www.chotot.com/mua-ban-dien-thoai?page=51\n",
            "Found 20 listings @ https://www.chotot.com/mua-ban-dien-thoai?page=52\n",
            "Found 20 listings @ https://www.chotot.com/mua-ban-dien-thoai?page=53\n",
            "Found 20 listings @ https://www.chotot.com/mua-ban-dien-thoai?page=56\n",
            "Found 20 listings @ https://www.chotot.com/mua-ban-dien-thoai?page=57\n",
            "Found 20 listings @ https://www.chotot.com/mua-ban-dien-thoai?page=55\n",
            "Found 20 listings @ https://www.chotot.com/mua-ban-dien-thoai?page=54\n",
            "Found 20 listings @ https://www.chotot.com/mua-ban-dien-thoai?page=58\n",
            "Found 20 listings @ https://www.chotot.com/mua-ban-dien-thoai?page=59\n",
            "Found 20 listings @ https://www.chotot.com/mua-ban-dien-thoai?page=60\n",
            "Found 20 listings @ https://www.chotot.com/mua-ban-dien-thoai?page=63\n",
            "Found 20 listings @ https://www.chotot.com/mua-ban-dien-thoai?page=64\n",
            "Found 20 listings @ https://www.chotot.com/mua-ban-dien-thoai?page=61\n",
            "Found 20 listings @ https://www.chotot.com/mua-ban-dien-thoai?page=65\n",
            "Found 20 listings @ https://www.chotot.com/mua-ban-dien-thoai?page=62\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The spider is consistently finding 20 listings on each page as well as successfully iterates through multiple pages (page=0 to page=62)."
      ],
      "metadata": {
        "id": "M6V8zXCiZCJB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are no visible errors or issues during the crawling process, which means the spider's methods (parse and parse_post) are executing without problems."
      ],
      "metadata": {
        "id": "slKdpJ80ZUVB"
      }
    }
  ]
}